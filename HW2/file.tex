\UseRawInputEncoding
\documentclass[12pt]{article}
\title{ECE C143A Homework 6}
\usepackage{subcaption}
\author{Lawrence Liu}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pdfpages}
\newcommand{\Laplace}{\mathscr{L}}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%
\usepackage{xcolor}
\usepackage{listings}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usepackage{amssymb}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour}}
\lstset{style=mystyle}

\begin{document}
\maketitle
\section*{Problem 1}
\subsection*{(a)}
False, the Na+ channel opens first
\subsection*{(b)}
False, only Na+ serve to depolarize the cell.
\subsection*{(c)}
True
\subsection*{(d)}
False, EEG's cannot record action potentials.
\subsection*{(e)}
False because $\lambda$ does not vary with time and a possion process is memoryless.
\subsection*{(f)}
False, if the Fano factor is greater than one, the firing variance is greater than the firing mean
\subsection*{(g)}
True
\subsection*{(h)}
False, the 
\subsection*{(i)}
False
\subsection*{(j)}

\subsection*{(m)}
False it is a low pass.
\subsection*{(n)}
False, good for visual bad for motor
\subsection*{(o)}
False, Absolute not relative


\section*{Problem 2}
\subsection*{(a)}
$f(\theta)$ reaces a max at $\theta=\theta_0$ therefore this
is the prefered direction.
\subsection*{(b)}
No because the values of the tuning curve would all be negative
\subsection*{(c)}
\begin{align*}
    \cos(\theta-\theta_0)&=e^{j(\theta-\theta_0)}\\
    &=(\cos(\theta)+j\sin(\theta))(\cos(\theta_0)-j\sin(\theta_0))\\
    &=\cos(theta)\cos(theta_0)+\sin(\theta)\sin(\theta_0)
\end{align*}
\subsection*{(d)}
$$k_0=c_0$$
$$k_1=c_1\sin(\theta_0)$$
$$k_2=c_1\cos(\theta_0)$$
\subsection*{(e)}
We have
$$y_0=25=k_0+k_2$$
$$y_{120}=70=k_0+\frac{k_1\sqrt{3}}{2}-\frac{k_2}{2}$$
$$y_{240}=10=k_0-\frac{k_2}{2}-\frac{k_1\sqrt{3}}{2}$$

Therefore we have
$$y_{120}+y_{240}=2k_0-k_2$$
$$2y_0-y_{120}-y_{240}=2k_0+2k_2-2k_0+k_2$$
$$k_2=\boxed{\frac{2y_0-y_{120}-y_{240}}{3}}$$
$$k_0=\boxed{\frac{y_0+y_{120}+y_{240}}{3}}$$
$$k_1=\boxed{\frac{y_{120}-y_{240}}{\sqrt{3}}}$$
\subsection*{(f) and (g)}
\includepdf[pages=-]{"Problem 2 Jupyter.pdf"}

\section*{Problem 3}

\section*{Problem 4}
\subsection*{(a)}
The ISI is a exponential distribution with paramter $\lambda$ so therefore the mean is
$$\frac{1}{\lambda}$$
\subsection*{(b)}
The probability that a given ISI is greater than the mean ISI is 
\begin{align*}
    \int_{\frac{1}{\lambda}}^{+\infty}\lambda e^{-\lambda t}dt&=\boxed{e^{-1}}
\end{align*}
\subsection*{(c)}
from bayes theorem we have
\begin{align*}
    Pr\left(T=t|T>\frac{1}{\lambda}\right)&=\frac{Pr(T>\frac{1}{\lambda}|T=t)f(T=t)}{Pr(T>\frac{1}{\lambda})} 
\end{align*}
\begin{align*}
    E[T|T>\frac{1}{\lambda}]&=\int_{0}^{\infty}tPr\left(T=t|T>\frac{1}{\lambda}\right)dt\\
    &=\int_{\frac{1}{\lambda}}^{\infty}te \lambda e^{-\lambda t} dt\\
    &=\boxed{\frac{e}{\lambda}}
\end{align*}

\subsection*{(d)}
we have
$$Pr\left(T=t|T<\frac{1}{\lambda}\right)=\frac{Pr(T<\frac{1}{\lambda}|T=t)f(T=t)}{Pr(T<\frac{1}{\lambda})}$$
therefore
\begin{align*}
    E[T|<>\frac{1}{\lambda}]&=\int_{0}^{\infty}tPr\left(T=t|T<\frac{1}{\lambda}\right)dt\\
    &=\int_{0}^{\frac{1}{\lambda}}t\frac{1}{1-e^{-1}} \lambda e^{-\lambda t} dt\\
    &=\boxed{\frac{1}{(1-e^{-1})\lambda}}
\end{align*}
\subsection*{(e)}
The probabalitiy for an ISI to be greater than the mean ISI is $e^{-1}$, therefore the 
number of spikes before one sees an ISI greater than the mean ISI is a geometric distribution
with $p=e^{-1}$. The mean of this distribution is $\frac{1}{p}=e$ therefore the expected number of spikes
that need to be fired before one sees an ISI less than the mean is $\boxed{e+1}$, since one spike needs to be fired first to measure the ISI.
\subsection*{(f)}
Let the expected waiting time be $T_w$ we have
$$T_w=\sum_{n=1}^\infty p(n) \sum_{i=1}^{n}E(T_i)$$
Where $T_i$ is the ith ISI time, and $p(n)$ is the probablity that the nth watiting time will be the
first greater than the mean. Thus $p(n)$ is a geometric distribution with $p=\frac{1}{e}$
$$E(T_w)=\sum_{n=1}^{\infty} p(n)\frac{n}{\lambda}$$
$$E(T_w)=\boxed{\frac{e}{\lambda}}$$
\section*{Problem 5}
\end{document}